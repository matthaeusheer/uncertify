{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from context import uncertify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uncertify.log import setup_logging\n",
    "setup_logging()\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Matplotlib DEBUG logging spits out a whole bunch of crap\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "plt.rc('font', family='serif')\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve, precision_recall_fscore_support\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "from uncertify.models.vae import load_vae_baur_model\n",
    "from uncertify.models.vae import VariationalAutoEncoder\n",
    "from uncertify.models.encoder_decoder_baur2020 import BaurDecoder, BaurEncoder\n",
    "from uncertify.data.dataloaders import dataloader_factory, DatasetType\n",
    "from uncertify.evaluation.inference import yield_inference_batches, yield_y_true_y_pred\n",
    "from uncertify.evaluation.ood_metrics import sample_wise_waic_scores\n",
    "from uncertify.visualization.grid import imshow_grid\n",
    "from uncertify.visualization.model_performance import plot_roc_curve, plot_precision_recall_curve, plot_confusion_matrix\n",
    "from uncertify.visualization.plotting import setup_plt_figure\n",
    "from uncertify.visualization.histograms import plot_multi_histogram\n",
    "from uncertify.data.datasets import GaussianNoiseDataset\n",
    "\n",
    "from uncertify.common import DATA_DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some paths and high level parameters\n",
    "RUN_DIR_PATH = Path('/media/juniors/2TB_internal_HD/lightning_logs/train_vae/')\n",
    "RUN_VERSIONS = [0, 1, 2]\n",
    "CHECKPOINT_PATHS = [RUN_DIR_PATH / f'version_{version}/checkpoints/last.ckpt' for version in RUN_VERSIONS]\n",
    "USE_N_BATCHES = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models = [load_vae_baur_model(path) for path in CHECKPOINT_PATHS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "HDD_PROCESSED_DIR_PATH = Path('/media/juniors/2TB_internal_HD/datasets/processed/')\n",
    "SSD_PROCESSED_DIR_PATH = DATA_DIR_PATH / 'processed'\n",
    "\n",
    "_, brats_val_t2_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, \n",
    "                                             val_set_path=SSD_PROCESSED_DIR_PATH / 'brats17_t2_hm_bc_std_bv-3.5.hdf5', shuffle_val=False)\n",
    "_, brats_val_t1_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, \n",
    "                                             val_set_path=SSD_PROCESSED_DIR_PATH / 'brats17_t1_hm_bc_std_bv-3.5.hdf5', shuffle_val=True)\n",
    "camcan_train_dataloader, camcan_val_dataloader = dataloader_factory(DatasetType.CAMCAN, batch_size=batch_size, \n",
    "                                                                    val_set_path=DATA_DIR_PATH / 'processed/camcan_val_t2_hm_std_bv3.5_xe.hdf5', \n",
    "                                                                    train_set_path=DATA_DIR_PATH / 'processed/camcan_train_t2_hm_std_bv3.5_xe.hdf5', \n",
    "                                                                    shuffle_val=False, shuffle_train=True)\n",
    "noise_set = GaussianNoiseDataset()\n",
    "noise_loader = DataLoader(noise_set, batch_size=batch_size)\n",
    "\n",
    "_, mnist_val_dataloader = dataloader_factory(DatasetType.MNIST, batch_size=batch_size, transform=torchvision.transforms.Compose([\n",
    "                                                                        torchvision.transforms.Resize((128, 128)),\n",
    "                                                                        torchvision.transforms.ToTensor()\n",
    "                                                                    ])\n",
    "                         )\n",
    "dataloader_dict = {\n",
    "    'BraTS T2 val': brats_val_t2_dataloader,\n",
    "    'BraTS T1 val': brats_val_t1_dataloader,\n",
    "    'CamCAN train': camcan_train_dataloader,\n",
    "    'Gaussian noise': noise_loader,\n",
    "    'MNIST': mnist_val_dataloader\n",
    "}\n",
    "\n",
    "for name, dataloader in dataloader_dict.items():\n",
    "    print(f'{name:15} dataloader: {len(dataloader)} batches (batch_size: {dataloader.batch_size}) -> {len(dataloader) * dataloader.batch_size} samples.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BACTHES = 10\n",
    "\n",
    "waic_dict = {}\n",
    "for name, data_loader in dataloader_dict.items():\n",
    "    LOG.info(f'WAIC score calculation for {name} ({NUM_BACTHES * data_loader.batch_size} patients)...')\n",
    "    waic_scores = sample_wise_waic_scores(models=ensemble_models, data_loader=data_loader, max_n_batches=NUM_BACTHES)\n",
    "    waic_dict[name] = waic_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, _ = plot_multi_histogram(waic_dict.values(), list(waic_dict.keys()), plot_density=False, \n",
    "                     figsize=(12, 6), xlabel='WAIC', ylabel='Slice-wise frequency',\n",
    "                     hist_kwargs={'bins': 15});\n",
    "fig.savefig(DATA_DIR_PATH / 'plots' / 'waic_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Test:\n",
    "    a: List[float] = field(default_factory=list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.a.extend([2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.evaluation.inference import AnomalyInferenceScores, AnomalyScores, SliceWiseAnomalyScores, SliceWiseCriteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    anomaly_scores = AnomalyInferenceScores(AnomalyScores(), \n",
    "                                            [SliceWiseAnomalyScores(criteria) for criteria in SliceWiseCriteria])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(anomaly_scores.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l = [\n",
    "        [\n",
    "            [[0, 1, 1],\n",
    "             [0, 0, 0],\n",
    "             [1, 0, 1]]\n",
    "        ],\n",
    "        [\n",
    "            [[0, 0, 1],\n",
    "             [0, 0, 0],\n",
    "             [1, 0, 1]]\n",
    "        ]\n",
    "]\n",
    "\n",
    "t = torch.tensor(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_abnormal_pixels = torch.sum(t > 0, axis=(1, 2, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_abnormal_pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.evaluation.inference import yield_inference_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in yield_inference_batches(brats_val_t2_dataloader, ensemble_models[0], max_batches=1):\n",
    "    s = torch.sum(batch.segmentation > 0, axis=(1, 2, 3))\n",
    "    y = batch.segmentation[batch.mask].flatten().numpy()\n",
    "    r = batch.segmentation[batch.mask].flatten()\n",
    "    print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(s.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertify-env",
   "language": "python",
   "name": "uncertify-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
