{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from context import uncertify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uncertify.log import setup_logging\n",
    "setup_logging()\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Matplotlib DEBUG logging spits out a whole bunch of crap\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms.transforms import Compose\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "plt.rc('font', family='serif')\n",
    "matplotlib.rcParams.update({'font.size': 18})\n",
    "\n",
    "from uncertify.data.np_transforms import NumpyReshapeTransform, Numpy2PILTransform\n",
    "from uncertify.data.datasets import GaussianNoiseDataset\n",
    "\n",
    "from uncertify.data.dataloaders import dataloader_factory, DatasetType\n",
    "\n",
    "from uncertify.evaluation.ood_metrics import sample_wise_waic_scores\n",
    "from uncertify.evaluation.ood_metrics import load_ensemble_models\n",
    "from uncertify.evaluation.evaluation_pipeline import run_ood_detection_performance, EvaluationConfig, EvaluationResult, PixelAnomalyDetectionResult, SliceAnomalyDetectionResults\n",
    "from uncertify.evaluation.evaluation_pipeline import print_results_from_evaluation_dirs\n",
    "from uncertify.evaluation.configs import OODDetectionResults\n",
    "from uncertify.utils.python_helpers import get_indices_of_n_largest_items, get_indices_of_n_smallest_items, get_idx_of_closest_value\n",
    "\n",
    "from uncertify.visualization.histograms import plot_multi_histogram\n",
    "from uncertify.visualization.ood_scores import plot_ood_scores\n",
    "from uncertify.visualization.grid import imshow_grid\n",
    "\n",
    "from uncertify.common import DATA_DIR_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some paths and high level parameters\n",
    "RUN_DIR_PATH = Path('/media/juniors/2TB_internal_HD/lightning_logs/train_vae/')\n",
    "RUN_VERSIONS = [0, 1, 2, 3]\n",
    "CHECKPOINT_PATHS = [RUN_DIR_PATH / f'version_{version}/checkpoints/last.ckpt' for version in RUN_VERSIONS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models = load_ensemble_models(DATA_DIR_PATH / 'ensemble_models', [f'model{idx}.ckpt' for idx in range(4)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 155\n",
    "USE_N_BATCHES = 10\n",
    "\n",
    "PROCESSED_DIR_PATH = Path('/media/juniors/2TB_internal_HD/datasets/processed/')\n",
    "\n",
    "brats_t2_path    = PROCESSED_DIR_PATH / 'brats17_t2_bc_std_bv3.5_l10.hdf5'\n",
    "brats_t2_hm_path = PROCESSED_DIR_PATH / 'brats17_t2_hm_bc_std_bv-3.5.hdf5'\n",
    "brats_t1_path    = PROCESSED_DIR_PATH / 'brats17_t1_bc_std_bv3.5_l10.hdf5'\n",
    "brats_t1_hm_path = PROCESSED_DIR_PATH / 'brats17_t1_hm_bc_std_bv-3.5.hdf5'\n",
    "camcan_t2_val_path   = DATA_DIR_PATH  / 'processed/camcan_val_t2_hm_std_bv3.5_xe.hdf5'\n",
    "camcan_t2_train_path = DATA_DIR_PATH  / 'processed/camcan_train_t2_hm_std_bv3.5_xe.hdf5'\n",
    "\n",
    "_, brats_val_t2_dataloader    = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, val_set_path=brats_t2_path, shuffle_val=False, num_workers=12)\n",
    "_, brats_val_t1_dataloader    = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, val_set_path=brats_t1_path, shuffle_val=False, num_workers=12)\n",
    "_, brats_val_t2_hm_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, val_set_path=brats_t2_hm_path, shuffle_val=False, num_workers=12)\n",
    "_, brats_val_t1_hm_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, val_set_path=brats_t1_hm_path, shuffle_val=False, num_workers=12)\n",
    "\n",
    "hflip_transform = torchvision.transforms.Compose([\n",
    "    NumpyReshapeTransform((200, 200)),\n",
    "    Numpy2PILTransform(),\n",
    "    torchvision.transforms.Resize((128, 128)),\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=1.0),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "vflip_transform = torchvision.transforms.Compose([\n",
    "    NumpyReshapeTransform((200, 200)),\n",
    "    Numpy2PILTransform(),\n",
    "    torchvision.transforms.Resize((128, 128)),\n",
    "    torchvision.transforms.RandomVerticalFlip(p=1.0),\n",
    "    torchvision.transforms.ToTensor()\n",
    "])\n",
    "\n",
    "_, brats_val_t2_hflip_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, val_set_path=brats_t2_path, shuffle_val=False, num_workers=12, transform=hflip_transform)\n",
    "_, brats_val_t2_vflip_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=batch_size, val_set_path=brats_t2_path, shuffle_val=False, num_workers=12, transform=vflip_transform)\n",
    "\n",
    "camcan_train_dataloader, camcan_val_dataloader = dataloader_factory(DatasetType.CAMCAN, batch_size=batch_size, val_set_path=camcan_t2_val_path, train_set_path=camcan_t2_train_path, shuffle_val=False, shuffle_train=True, num_workers=12)\n",
    "\n",
    "noise_set = GaussianNoiseDataset()\n",
    "noise_loader = DataLoader(noise_set, batch_size=batch_size)\n",
    "\n",
    "_, mnist_val_dataloader = dataloader_factory(DatasetType.MNIST, batch_size=batch_size, transform=torchvision.transforms.Compose([\n",
    "                                                                        torchvision.transforms.Resize((128, 128)),\n",
    "                                                                        torchvision.transforms.ToTensor()\n",
    "                                                                    ])\n",
    "                         )\n",
    "\n",
    "dataloader_dict = {#'BraTS T2 val': brats_val_t2_dataloader, \n",
    "                   #'BraTS T1 val': brats_val_t1_dataloader, \n",
    "                   #'BraTS T2 HM val': brats_val_t2_hm_dataloader, \n",
    "                   #'BraTS T1 HM val': brats_val_t1_hm_dataloader,\n",
    "                   #'CamCAN train': camcan_train_dataloader,\n",
    "                   #'Gaussian noise': noise_loader,\n",
    "                   #'MNIST': mnist_val_dataloader,\n",
    "                   'BraTS T2 HFlip': brats_val_t2_hflip_dataloader,\n",
    "                   'BraTS T2 VFlip': brats_val_t2_vflip_dataloader\n",
    "}\n",
    "brats_dataloader_dict = {key: val for key, val in dataloader_dict.items() if 'BraTS' in key}\n",
    "\n",
    "for name, dataloader in dataloader_dict.items(): \n",
    "    print(f'{name:15} dataloader: {len(dataloader)} batches (batch_size: {dataloader.batch_size}) -> {len(dataloader) * dataloader.batch_size} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOD Detection Evaluation for different OOD datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_numbers = []\n",
    "for name, dataloader in dataloader_dict.items():\n",
    "    LOG.info(f'OOD evaluation for {name}...')\n",
    "    eval_cfg = EvaluationConfig()\n",
    "    eval_cfg.do_plots = True\n",
    "    eval_cfg.use_n_batches = USE_N_BATCHES\n",
    "    results = EvaluationResult(DATA_DIR_PATH / 'evaluation', eval_cfg, PixelAnomalyDetectionResult(), SliceAnomalyDetectionResults(), OODDetectionResults())\n",
    "    results.pixel_anomaly_result.best_threshold = 1.35\n",
    "    results.make_dirs()\n",
    "    run_numbers.append(results.run_number)\n",
    "    run_ood_detection_performance(ensemble_models, camcan_train_dataloader, dataloader, eval_cfg, results)\n",
    "    results.test_set_name = name\n",
    "    results.to_json()\n",
    "print_results_from_evaluation_dirs(DATA_DIR_PATH / 'evaluation', run_numbers, print_results_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WAIC Score Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BACTHES = 10\n",
    "\n",
    "dataloader_dict = {'BraTS T2 val': brats_val_t2_dataloader, \n",
    "                   #'BraTS T1 val': brats_val_t1_dataloader, \n",
    "                   'BraTS T2 HM val': brats_val_t2_hm_dataloader, \n",
    "                   #'BraTS T1 HM val': brats_val_t1_hm_dataloader,\n",
    "                   'CamCAN train': camcan_train_dataloader,\n",
    "                   #'Gaussian noise': noise_loader,\n",
    "                   #'MNIST': mnist_val_dataloader\n",
    "}\n",
    "\n",
    "waic_dict = {}\n",
    "for name, data_loader in dataloader_dict.items():\n",
    "    LOG.info(f'WAIC score calculation for {name} ({NUM_BACTHES * data_loader.batch_size} slices)...')\n",
    "    slice_wise_waic_scores, slice_wise_is_lesional, scans = sample_wise_waic_scores(models=ensemble_models, data_loader=data_loader, max_n_batches=NUM_BACTHES, return_slices=True)\n",
    "    \n",
    "    # Organise as healthy / unhealthy\n",
    "    healthy_waics = []\n",
    "    lesional_waics = []\n",
    "    healthy_scans = []\n",
    "    lesional_scans = []\n",
    "    for idx in range(len(slice_wise_waic_scores)):\n",
    "        is_lesional_slice = slice_wise_is_lesional[idx]\n",
    "        if is_lesional_slice:\n",
    "            lesional_waics.append(slice_wise_waic_scores[idx])\n",
    "            if scans is not None:\n",
    "                lesional_scans.append(scans[idx])\n",
    "        else:\n",
    "            healthy_waics.append(slice_wise_waic_scores[idx])\n",
    "            if scans is not None:\n",
    "                healthy_scans.append(scans[idx])\n",
    "\n",
    "    dataset_waic_dict = {'all': slice_wise_waic_scores, 'healthy': healthy_waics, 'lesional': lesional_waics,\n",
    "                         'healthy_scans': healthy_scans, 'lesional_scans': lesional_scans}\n",
    "    waic_dict[name] = dataset_waic_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_ood_scores(waic_dict, score_label='WAIC', dataset_name_filters=['BraTS T2 val'], modes_to_include=['healthy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "waic_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'BraTS T2 val'\n",
    "\n",
    "\n",
    "def plot_most_least_ood(waic_dict: dict, dataset_name: str, n_most: int = 16, do_lesional: bool = True) -> None:\n",
    "    \"\"\"For healthy and lesional samples, plot the ones which are most and least OOD.\"\"\"\n",
    "    ood_dict = waic_dict[dataset_name]\n",
    "    \n",
    "    def create_ood_grids(healthy_leasional: str):\n",
    "        scores = ood_dict[healthy_leasional]\n",
    "        slices = ood_dict[f'{healthy_leasional}_scans']\n",
    "        largest_score_indices = get_indices_of_n_largest_items(scores, n_most)\n",
    "        smallest_score_indices = get_indices_of_n_smallest_items(scores, n_most)\n",
    "        \n",
    "        largest_slices = [slices[idx] for idx in largest_score_indices]\n",
    "        smallest_slices = [slices[idx] for idx in smallest_score_indices]\n",
    "        \n",
    "        largest_grid = torchvision.utils.make_grid(largest_slices, padding=0, normalize=False)\n",
    "        smallest_grid = torchvision.utils.make_grid(smallest_slices, padding=0, normalize=False)\n",
    "        \n",
    "        return largest_grid, smallest_grid\n",
    "    \n",
    "    print('Creating healthy grids...')\n",
    "    most_ood_healthy_grid, least_ood_healthy_grid = create_ood_grids('healthy')\n",
    "    if do_lesional:\n",
    "        print('Creating lesional grids...')\n",
    "        most_ood_lesional_grid, least_ood_lesional_grid = create_ood_grids('lesional')\n",
    "    \n",
    "    imshow_grid(most_ood_healthy_grid, one_channel=True, figsize=(12, 8), title=f'Most OOD Healthy {dataset_name}', axis='off')\n",
    "    imshow_grid(least_ood_healthy_grid, one_channel=True, figsize=(12, 8), title=f'Least OOD Healthy {dataset_name}', axis='off')\n",
    "    if do_lesional:\n",
    "        imshow_grid(most_ood_lesional_grid, one_channel=True, figsize=(12, 8), title=f'Most OOD Lesional {dataset_name}', axis='off')\n",
    "        imshow_grid(least_ood_lesional_grid, one_channel=True, figsize=(12, 8), title=f'Least OOD Lesional {dataset_name}', axis='off')\n",
    "\n",
    "plot_most_least_ood(waic_dict, 'BraTS T2 val')\n",
    "plot_most_least_ood(waic_dict, 'BraTS T2 HM val')\n",
    "plot_most_least_ood(waic_dict, 'CamCAN train', do_lesional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples_close_to_score(ood_dict: dict, dataset_name: str, min_score: float, max_score: float, n: int = 32, do_lesional: bool = True) -> None:\n",
    "    ood_dict = ood_dict[dataset_name]\n",
    "    ref_scores = np.linspace(min_score, max_score, n)\n",
    "    def create_ood_grids(healthy_leasional: str):\n",
    "        scores = ood_dict[healthy_leasional]\n",
    "        slices = ood_dict[f'{healthy_leasional}_scans']\n",
    "        \n",
    "        final_scores = []\n",
    "        final_slices = []\n",
    "        \n",
    "        for ref_score in ref_scores:\n",
    "            scores_idx = get_idx_of_closest_value(scores, ref_score)\n",
    "            final_scores.append(scores[scores_idx])\n",
    "            final_slices.append(slices[scores_idx])\n",
    "            \n",
    "        return torchvision.utils.make_grid(final_slices, padding=0, normalize=False)\n",
    "    \n",
    "    healthy_grid = create_ood_grids('healthy')\n",
    "    if do_lesional:\n",
    "        lesional_grid = create_ood_grids('lesional')\n",
    "    \n",
    "    imshow_grid(healthy_grid, one_channel=True, figsize=(12, 8), title=f'Healthy {dataset_name} {min_score}-{max_score}', axis='off')\n",
    "    if do_lesional:\n",
    "        imshow_grid(lesional_grid, one_channel=True, figsize=(12, 8), title=f'Lesional {dataset_name} {min_score}-{max_score}', axis='off')\n",
    "\n",
    "plot_samples_close_to_score(waic_dict, 'BraTS T2 val', min_score=0.95, max_score=1.2)\n",
    "plot_samples_close_to_score(waic_dict, 'BraTS T2 HM val', min_score=0.85, max_score=1.2)\n",
    "plot_samples_close_to_score(waic_dict, 'CamCAN train', do_lesional=False, min_score=0.8, max_score=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertify-env",
   "language": "python",
   "name": "uncertify-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
