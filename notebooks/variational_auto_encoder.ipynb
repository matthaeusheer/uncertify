{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from context import uncertify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uncertify.log import setup_logging\n",
    "setup_logging()\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Matplotlib DEBUG logging spits out a whole bunch of crap\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from uncertify.tutorials.variational_auto_encoder import VariationalAutoEncoder\n",
    "from uncertify.tutorials.variational_auto_encoder import train_vae, visualize_reconstructions, visualize_generated\n",
    "from uncertify.common import DATA_DIR_PATH\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data_loaders(transform: transforms.Compose,\n",
    "                             data_path: Path,\n",
    "                             batch_size: int,\n",
    "                             num_workers: int) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_set = torchvision.datasets.MNIST(root=data_path,\n",
    "                                             train=True,\n",
    "                                             download=True,\n",
    "                                             transform=transform)\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "    test_set = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "    test_loader = DataLoader(test_set,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=num_workers)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_loader, test_loader = get_mnist_data_loaders(transform=transform,\n",
    "                                                   data_path=DATA_DIR_PATH / 'mnist_data',\n",
    "                                                   batch_size=64,\n",
    "                                                   num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 30\n",
    "LEARNING_RATE = 0.00003\n",
    "PRINT_STEPS = 200\n",
    "N_Z_SAMPLES = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = VariationalAutoEncoder(input_dim=784, hidden_dim=128, bottleneck_dim=20)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "sampled_z = torch.randn(N_Z_SAMPLES, model.bottleneck_dim).cuda()\n",
    "trained_model = train_vae(model, device, train_loader, test_loader, optimizer, N_EPOCHS, 1, sampled_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figs = visualize_reconstructions(trained_model, test_loader, device, n_batches=1, max_samples=5, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruct_random_z():\n",
    "    sampled_z = torch.randn(16, model.bottleneck_dim).cuda()\n",
    "    generated = model._decode(sampled_z).view(-1, 28, 28).cpu().detach().numpy()\n",
    "    fig = visualize_generated(generated)\n",
    "reconstruct_random_z()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertify_env",
   "language": "python",
   "name": "uncertify_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
