{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from context import uncertify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "from uncertify.log import setup_logging\n",
    "setup_logging()\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Matplotlib DEBUG logging spits out a whole bunch of crap\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import itertools\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from uncertify.tutorials.auto_encoder import AutoEncoder\n",
    "from uncertify.common import DATA_DIR_PATH\n",
    "\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mnist_data_loaders(transform: transforms.Compose,\n",
    "                             data_path: Path,\n",
    "                             batch_size: int,\n",
    "                             num_workers: int) -> Tuple[DataLoader, DataLoader]:\n",
    "    train_set = torchvision.datasets.MNIST(root=data_path,\n",
    "                                             train=True,\n",
    "                                             download=True,\n",
    "                                             transform=transform)\n",
    "    train_loader = DataLoader(train_set,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=num_workers)\n",
    "\n",
    "    test_set = torchvision.datasets.MNIST(root=data_path,\n",
    "                                            train=False,\n",
    "                                            download=True,\n",
    "                                            transform=transform)\n",
    "    test_loader = DataLoader(test_set,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=num_workers)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_loader, test_loader = get_mnist_data_loaders(transform=transform,\n",
    "                                                   data_path=DATA_DIR_PATH / 'mnist_data',\n",
    "                                                   batch_size=4,\n",
    "                                                   num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LEARNING_RATE = 0.01\n",
    "PRINT_STEPS = 1000\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoEncoder(input_dim=784,\n",
    "        latent_dim=128,\n",
    "        encoder_hidden_dims=[512, 265])\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=0.9)\n",
    "\n",
    "def train(model, device, train_loader):\n",
    "    model = model.to(device)\n",
    "    for epoch_idx in range(N_EPOCHS):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (batch_features, _) in enumerate(train_loader):\n",
    "            batch_flat_features = batch_features.view(-1, 784).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model.forward(batch_flat_features)\n",
    "            loss = criterion(outputs, batch_flat_features)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            if (batch_idx + 1) % PRINT_STEPS == 0:\n",
    "                print(f'epoch {epoch_idx + 1:<2} | batch {batch_idx + 1:5}  >>>  loss: {running_loss / PRINT_STEPS:.3f}')\n",
    "                running_loss = 0.0\n",
    "    return model\n",
    "\n",
    "trained_model = train(model, device, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_reconstructions(trained_model, test_loader, cmap='hot', n_batches=1):\n",
    "    plt.set_cmap(cmap)\n",
    "    with torch.no_grad():\n",
    "        for batch_features, _ in itertools.islice(test_loader, n_batches):\n",
    "            batch_flat_feature = batch_features.view(-1, 784)\n",
    "            outputs = trained_model.forward(batch_flat_feature.to(device))\n",
    "            for in_feature, out in zip(batch_features, outputs):\n",
    "                out_np = out.view(28, 28).cpu().numpy()\n",
    "                fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "                ax1.imshow(in_feature.view(28, 28).numpy())\n",
    "                ax2.imshow(out_np)\n",
    "                ax1.set_axis_off()\n",
    "                ax2.set_axis_off()\n",
    "                \n",
    "visualize_reconstructions(trained_model, test_loader, n_batches=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertify",
   "language": "python",
   "name": "uncertify"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}