{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from context import uncertify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uncertify.log import setup_logging\n",
    "setup_logging()\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Matplotlib DEBUG logging spits out a whole bunch of crap\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.ERROR)\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from uncertify.data.dataloaders import dataloader_factory, DatasetType\n",
    "from uncertify.visualization.reconstruction import plot_stacked_scan_reconstruction_batches\n",
    "from uncertify.evaluation.inference import yield_inference_batches, yield_anomaly_predictions\n",
    "from uncertify.evaluation.utils import residual_l1, residual_l1_max\n",
    "from uncertify.visualization.plotting import save_fig\n",
    "from uncertify.data.datasets import GaussianNoiseDataset\n",
    "from uncertify.data.default_dataloaders import default_dataloader_dict_factory, filter_dataloader_dict\n",
    "from uncertify.io.models import load_ensemble_models, load_vae_baur_model\n",
    "from uncertify.evaluation.evaluation_pipeline import run_anomaly_detection_performance\n",
    "\n",
    "from uncertify.common import DATA_DIR_PATH, HD_DATA_PATH, HD_MODELS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load (ensemble) models\n",
    "RUN_VERSIONS = [0, 1, 2, 3, 4]\n",
    "ensemble_models = load_ensemble_models(HD_MODELS_PATH / 'scheduled_masked_ensembles', [f'model_{idx}.ckpt' for idx in RUN_VERSIONS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model_dir_path = HD_MODELS_PATH / 'scheduled_masked_ensembles'\n",
    "file_names = [f'model_{idx}.ckpt' for idx in [0, 1, 2, 3, 4]]\n",
    "\n",
    "ensembles = load_ensemble_models(dir_path=model_dir_path, file_names=file_names)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = load_vae_baur_model(Path('/mnt/2TB_internal_HD/lightning_logs/beta_test/version_2/checkpoints/last.ckpt'))\n",
    "model = load_vae_baur_model(Path('/mnt/1TB_SSD/lightning_logs/beta_test/version_2/checkpoints/last.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_dict = default_dataloader_dict_factory(batch_size=128, num_workers=12, shuffle_val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Infernce Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_batches = 1\n",
    "\n",
    "#plot_dataloader_dict = filter_dataloader_dict(dataloader_dict, contains=['BraTS'], exclude=[])\n",
    "plot_dataloader_dict = {name: dataloader_dict[name] for name in ['CamCAN train', 'BraTS T2 val']}\n",
    "\n",
    "for dataloader_name, dataloader in plot_dataloader_dict.items():\n",
    "    print(f'Loader {dataloader_name}, Dataset: {dataloader.dataset.name}')\n",
    "    batch_generator = yield_inference_batches(dataloader, model, residual_fn=residual_l1_max, residual_threshold=0.60,\n",
    "                                              manual_seed_val=None)\n",
    "    plot_stacked_scan_reconstruction_batches(batch_generator, plot_n_batches, nrow=32,\n",
    "                                             cmap='gray', axis='off', figsize=(15, 15), mask_background=False,\n",
    "                                             save_dir_path=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-Wise Anomaly Detection Performance (ROC & PRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.evaluation.configs import EvaluationConfig, EvaluationResult\n",
    "from uncertify.evaluation.evaluation_pipeline import OUT_DIR_PATH, PixelAnomalyDetectionResult, SliceAnomalyDetectionResults, OODDetectionResults, print_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cfg = EvaluationConfig()\n",
    "eval_cfg.use_n_batches = 20\n",
    "\n",
    "eval_dataloader = dataloader_dict['BraTS T2 HM val']\n",
    "\n",
    "results = EvaluationResult(OUT_DIR_PATH, eval_cfg, PixelAnomalyDetectionResult(), SliceAnomalyDetectionResults(), OODDetectionResults())\n",
    "results.make_dirs()\n",
    "results.pixel_anomaly_result.best_threshold = 0.65\n",
    "\n",
    "results = run_anomaly_detection_performance(eval_cfg, model, eval_dataloader, results)\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.evaluation.model_performance import mean_std_dice_scores, mean_std_iou_scores\n",
    "from uncertify.visualization.model_performance import plot_segmentation_performance_vs_threshold\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Only run with one pre-defined threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_batches = 20\n",
    "residual_threshold = 0.67\n",
    "\n",
    "eval_dataloader = dataloader_dict['BraTS T2 val']\n",
    "best_mean_dice_score, best_std_dice_score = mean_std_dice_scores(eval_dataloader, \n",
    "                                                                 model,\n",
    "                                                                 [residual_threshold],\n",
    "                                                                 max_n_batches)\n",
    "LOG.info(f'Dice score (t={residual_threshold:.2f}) for {eval_dataloader.dataset.name}: '\n",
    "         f'{best_mean_dice_score[0]:.2f} +- {best_std_dice_score[0]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check over multiple thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_thresholds = 8\n",
    "max_n_batches = 10\n",
    "\n",
    "pixel_thresholds = np.linspace(0.4, 1.4, n_thresholds)\n",
    "eval_dataloader = dataloader_dict['BraTS T2 HM val']\n",
    "mean_dice_scores, std_dice_scores = mean_std_dice_scores(eval_dataloader, model, residual_thresholds=pixel_thresholds, max_n_batches=max_n_batches)\n",
    "best_dice_idx, best_dice_score = max(enumerate(mean_dice_scores), key=operator.itemgetter(1))\n",
    "print(f'Best dice score: {best_dice_score:.2f}+-{std_dice_scores[best_dice_idx]} with threshold {pixel_thresholds[best_dice_idx]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_segmentation_performance_vs_threshold(pixel_thresholds, dice_scores=mean_dice_scores, dice_stds=std_dice_scores, iou_scores=None, \n",
    "                                                    train_set_threshold=None, figsize=(12, 6));\n",
    "fig.savefig(DATA_DIR_PATH / 'plots' / 'dice_iou_vs_threshold.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample-wise Loss Term Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from uncertify.visualization.histograms import plot_loss_histograms\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_batches = 10\n",
    "redisual_threshold = 0.67\n",
    "\n",
    "select_dataloaders = ['CamCAN train', 'BraTS T2 val']\n",
    "\n",
    "output_generators = []\n",
    "for dataloader_name in select_dataloaders:\n",
    "    dataloader = dataloader_dict[dataloader_name]\n",
    "    output_generators.append(yield_inference_batches(dataloader, model, max_n_batches, redisual_threshold, progress_bar_suffix=f'{dataloader_name}',\n",
    "                                                    manual_seed_val=10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_axes = plot_loss_histograms(output_generators=output_generators, names=select_dataloaders, \n",
    "                                 figsize=(17, 3.5), ylabel='Frequency', plot_density=True, show_data_ticks=False, \n",
    "                                 kde_bandwidth=[0.009, 0.009*5.5], show_histograms=False)\n",
    "\n",
    "for idx, (fig, _) in enumerate(figs_axes):\n",
    "    save_fig(fig, DATA_DIR_PATH / 'plots' / f'loss_term_distributions_{idx}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.visualization.threshold_search import plot_fpr_vs_residual_threshold\n",
    "from uncertify.evaluation.evaluation_pipeline import run_residual_threshold_evaluation, EvaluationResult, PixelAnomalyDetectionResult, SliceAnomalyDetectionResults, OODDetectionResults\n",
    "from uncertify.evaluation.configs import EvaluationConfig, PixelThresholdSearchConfig\n",
    "from uncertify.evaluation.evaluation_pipeline import OUT_DIR_PATH\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "eval_cfg = EvaluationConfig()\n",
    "eval_cfg.use_n_batches = 10\n",
    "results = EvaluationResult(OUT_DIR_PATH, eval_cfg, PixelAnomalyDetectionResult(), SliceAnomalyDetectionResults(), OODDetectionResults())\n",
    "results.make_dirs()\n",
    "\n",
    "results = run_residual_threshold_evaluation(model, dataloader_dict['CamCAN train'], eval_cfg, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot MNIST reconstructions\n",
    "Run various MNIST examples (batches consisting of samples of a certain number) through the model and plot input and reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_batches = 1\n",
    "batch_size = 8\n",
    "for n in range(0, 10):\n",
    "    _, mnist_val_dataloader = dataloader_factory(DatasetType.MNIST, \n",
    "                                                 batch_size=batch_size, \n",
    "                                                 transform=torchvision.transforms.Compose([\n",
    "                                                                        torchvision.transforms.Resize((128, 128)),\n",
    "                                                                        torchvision.transforms.ToTensor()]),\n",
    "                                                 mnist_label=n)\n",
    "    batch_generator = yield_inference_batches(mnist_val_dataloader, model, residual_threshold=1.8)\n",
    "    plot_stacked_scan_reconstruction_batches(batch_generator, plot_n_batches, \n",
    "                                             cmap='hot', axis='off', figsize=(15, 15), save_dir_path=DATA_DIR_PATH/'reconstructions')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uncertify-env",
   "language": "python",
   "name": "uncertify-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
