{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from context import uncertify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uncertify.log import setup_logging\n",
    "setup_logging()\n",
    "LOG = logging.getLogger(__name__)\n",
    "\n",
    "# Matplotlib DEBUG logging spits out a whole bunch of crap\n",
    "mpl_logger = logging.getLogger('matplotlib')\n",
    "mpl_logger.setLevel(logging.WARNING)\n",
    "numba_logger = logging.getLogger('numba')\n",
    "numba_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import operator\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from tqdm import tqdm\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from uncertify.data.dataloaders import dataloader_factory, DatasetType\n",
    "from uncertify.visualization.reconstruction import plot_stacked_scan_reconstruction_batches\n",
    "from uncertify.evaluation.inference import yield_inference_batches, yield_anomaly_predictions\n",
    "from uncertify.evaluation.utils import residual_l1, residual_l1_max\n",
    "from uncertify.visualization.plotting import save_fig\n",
    "from uncertify.data.datasets import GaussianNoiseDataset\n",
    "from uncertify.io.models import load_ensemble_models\n",
    "from uncertify.models.vae import load_vae_baur_model\n",
    "from uncertify.evaluation.evaluation_pipeline import run_anomaly_detection_performance\n",
    "\n",
    "from uncertify.common import DATA_DIR_PATH, HD_DATA_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "MASKED_TRAINING_MODELS = True\n",
    "model_dir = 'masked_ensemble_models' if MASKED_TRAINING_MODELS else 'ensemble_models'\n",
    "RUN_VERSIONS = [1, 2, 3, 4, 5] if MASKED_TRAINING_MODELS else [1, 2, 3, 4, 5]\n",
    "ensemble_models = load_ensemble_models(DATA_DIR_PATH / 'masked_ensemble_models', [f'model{idx}.ckpt' for idx in RUN_VERSIONS])\n",
    "model = ensemble_models[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_vae_baur_model(HD_DATA_PATH.parent / 'lightning_logs/schedule_test/version_4/checkpoints/last.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8\n",
    "num_workers = 0\n",
    "SHUFFLE_VAL = True\n",
    "\n",
    "EVAL_DIR_PATH = DATA_DIR_PATH / 'evaluation'\n",
    "PROCESSED_DIR_PATH = HD_DATA_PATH / 'processed' \n",
    "\n",
    "brats_t2_path    = DATA_DIR_PATH  / 'processed/brats17_t2_bc_std_bv3.5.hdf5'\n",
    "brats_t2_hm_path = PROCESSED_DIR_PATH / 'brats17_t2_hm_bc_std_bv3.5.hdf5'\n",
    "brats_t1_path    = DATA_DIR_PATH  / 'processed/brats17_t1_bc_std_bv3.5.hdf5'\n",
    "brats_t1_hm_path = PROCESSED_DIR_PATH / 'brats17_t1_hm_bc_std_bv-3.5.hdf5'\n",
    "camcan_t2_val_path   = DATA_DIR_PATH  / 'processed/camcan_val_t2_hm_std_bv3.5_xe.hdf5'\n",
    "camcan_t2_train_path = DATA_DIR_PATH  / 'processed/camcan_train_t2_hm_std_bv3.5_xe.hdf5'\n",
    "\n",
    "_, brats_val_t2_dataloader    = dataloader_factory(DatasetType.BRATS17, batch_size=BATCH_SIZE, val_set_path=brats_t2_path, shuffle_val=SHUFFLE_VAL, num_workers=num_workers)\n",
    "_, brats_val_t1_dataloader    = dataloader_factory(DatasetType.BRATS17, batch_size=BATCH_SIZE, val_set_path=brats_t1_path, shuffle_val=SHUFFLE_VAL, num_workers=num_workers)\n",
    "_, brats_val_t2_hm_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=BATCH_SIZE, val_set_path=brats_t2_hm_path, shuffle_val=SHUFFLE_VAL, num_workers=num_workers)\n",
    "_, brats_val_t1_hm_dataloader = dataloader_factory(DatasetType.BRATS17, batch_size=BATCH_SIZE, val_set_path=brats_t1_hm_path, shuffle_val=SHUFFLE_VAL, num_workers=num_workers)\n",
    "\n",
    "camcan_train_dataloader, camcan_val_dataloader = dataloader_factory(DatasetType.CAMCAN, batch_size=BATCH_SIZE, \n",
    "                                                                    val_set_path=camcan_t2_val_path, train_set_path=camcan_t2_train_path, \n",
    "                                                                    shuffle_val=SHUFFLE_VAL, shuffle_train=True, num_workers=num_workers)\n",
    "camcan_lesional_train_dataloader, camcan_lesional_val_dataloader = dataloader_factory(DatasetType.CAMCAN, batch_size=BATCH_SIZE, val_set_path=camcan_t2_val_path, \n",
    "                                                                                      train_set_path=camcan_t2_train_path, shuffle_val=SHUFFLE_VAL, shuffle_train=SHUFFLE_VAL, add_gauss_blobs=True)\n",
    "\n",
    "noise_set = GaussianNoiseDataset()\n",
    "noise_loader = DataLoader(noise_set, batch_size=BATCH_SIZE)\n",
    "\n",
    "_, mnist_val_dataloader = dataloader_factory(DatasetType.MNIST, batch_size=BATCH_SIZE, transform=torchvision.transforms.Compose([\n",
    "                                                                        torchvision.transforms.Resize((128, 128)),\n",
    "                                                                        torchvision.transforms.ToTensor()\n",
    "                                                                    ])\n",
    "                         )\n",
    "\n",
    "for name, dataloader in [('BraTS T2 val', brats_val_t2_dataloader), \n",
    "                         ('BraTS T1 val', brats_val_t1_dataloader), \n",
    "                         ('BraTS T2 HM val', brats_val_t2_hm_dataloader), \n",
    "                         ('BraTS T1 HM val', brats_val_t1_hm_dataloader),\n",
    "                         ('CamCAN train', camcan_train_dataloader),\n",
    "                         ('Gaussian noise', noise_loader),\n",
    "                         ('MNIST', mnist_val_dataloader)\n",
    "                        ]: \n",
    "    print(f'{name:15} dataloader: {len(dataloader)} batches (batch_size: {dataloader.batch_size}) -> {len(dataloader) * dataloader.batch_size} samples.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Infernce Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_batches = 5\n",
    "\n",
    "dataloaders = [#brats_val_t2_dataloader, \n",
    "                brats_val_t2_hm_dataloader,\n",
    "                #brats_val_t1_hm_dataloader, \n",
    "                #brats_val_t1_dataloader, \n",
    "                #noise_loader, \n",
    "                #mnist_val_dataloader\n",
    "                camcan_train_dataloader\n",
    "               ]\n",
    "\n",
    "for dataloader in dataloaders:\n",
    "    print(f'Dataset: {dataloader.dataset.name}')\n",
    "    batch_generator = yield_inference_batches(dataloader, model, residual_fn=residual_l1, residual_threshold=0.90)\n",
    "    plot_stacked_scan_reconstruction_batches(batch_generator, plot_n_batches, nrow=8,\n",
    "                                             cmap='gray', axis='off', figsize=(15, 15),\n",
    "                                             save_dir_path=DATA_DIR_PATH/'reconstructions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pixel-Wise Anomaly Detection Performance (ROC & PRC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.evaluation.configs import EvaluationConfig, EvaluationResult\n",
    "from uncertify.evaluation.evaluation_pipeline import OUT_DIR_PATH, PixelAnomalyDetectionResult, SliceAnomalyDetectionResults, OODDetectionResults, print_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_cfg = EvaluationConfig()\n",
    "eval_cfg.use_n_batches = 20\n",
    "\n",
    "results = EvaluationResult(OUT_DIR_PATH, eval_cfg, PixelAnomalyDetectionResult(), SliceAnomalyDetectionResults(), OODDetectionResults())\n",
    "results.make_dirs()\n",
    "results.pixel_anomaly_result.best_threshold = 0.95\n",
    "\n",
    "results = run_anomaly_detection_performance(eval_cfg, model, brats_val_t2_hm_dataloader, results)\n",
    "print_results(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.evaluation.model_performance import mean_std_dice_scores, mean_std_iou_scores\n",
    "from uncertify.visualization.model_performance import plot_segmentation_performance_vs_threshold\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best dice score: {best_dice_score:.2f}+-{std_dice_scores[best_dice_idx]} with threshold {pixel_thresholds[best_dice_idx]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_thresholds = 10\n",
    "max_n_batches = 10\n",
    "\n",
    "pixel_thresholds = np.linspace(0.0, 3.0, n_thresholds)\n",
    "mean_dice_scores, std_dice_scores = mean_std_dice_scores(brats_val_t2_hm_dataloader, model, residual_thresholds=pixel_thresholds, max_n_batches=max_n_batches)\n",
    "best_dice_idx, best_dice_score = max(enumerate(mean_dice_scores), key=operator.itemgetter(1))\n",
    "print(f'Best dice score: {best_dice_score:.2f}+-{std_dice_scores[best_dice_idx]} with threshold {pixel_thresholds[best_dice_idx]}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_segmentation_performance_vs_threshold(pixel_thresholds, dice_scores=mean_dice_scores, dice_stds=std_dice_scores, iou_scores=None, \n",
    "                                                    train_set_threshold=None, figsize=(12, 6));\n",
    "fig.savefig(DATA_DIR_PATH / 'plots' / 'dice_iou_vs_threshold.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample-wise Loss Term Histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "from uncertify.visualization.histograms import plot_loss_histograms\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_batches = 1\n",
    "\n",
    "dataloaders = [camcan_train_dataloader,\n",
    "               brats_val_t2_dataloader, \n",
    "               brats_val_t1_dataloader, \n",
    "               mnist_val_dataloader,\n",
    "               noise_loader\n",
    "              ]\n",
    "\n",
    "generator_names = ['CamCAN T2', \n",
    "                   'BraTS17 T2',\n",
    "                   'BraTS17 T1',\n",
    "                   'MNIST',\n",
    "                   'Gaussian Noise'\n",
    "                  ]\n",
    "\n",
    "output_generators = []\n",
    "for dataloader, name in zip(dataloaders, generator_names):\n",
    "    output_generators.append(yield_inference_batches(dataloader, model, max_n_batches, progress_bar_suffix=f'{name}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs_axes = plot_loss_histograms(output_generators=output_generators, names=generator_names, \n",
    "                                 figsize=(10, 3), ylabel='Frequency', plot_density=True, show_data_ticks=False, kde_bandwidth=[0.009, 0.009*5.5], show_histograms=False)\n",
    "\n",
    "for idx, (fig, _) in enumerate(figs_axes):\n",
    "    save_fig(fig, DATA_DIR_PATH / 'plots' / f'loss_term_distributions_{idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.visualization.latent_space_analysis import plot_umap_latent_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_n_batches = 10\n",
    "redisual_threshold = 1.8\n",
    "\n",
    "dataloaders = [brats_val_t1_dataloader, \n",
    "               brats_val_t2_dataloader, \n",
    "               #mnist_val_dataloader,\n",
    "               #noise_loader,\n",
    "               camcan_train_dataloader,\n",
    "]\n",
    "\n",
    "generator_names = ['BraTS17 T1',\n",
    "                   'BraTS17 T2',\n",
    "                   #'MNIST Val',\n",
    "                   #'Gaussian Noise',\n",
    "                   'CamCAN Train T2']\n",
    "\n",
    "output_generators = []\n",
    "for dataloader, name in zip(dataloaders, generator_names):\n",
    "    output_generators.append(yield_inference_batches(dataloader, model, max_n_batches, redisual_threshold, progress_bar_suffix=f'{name}'))\n",
    "\n",
    "umap_fig = plot_umap_latent_embedding(output_generators, generator_names, figsize=(14, 10))\n",
    "umap_fig.savefig(DATA_DIR_PATH / 'plots' / f'umap_latent_embedding.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.visualization.threshold_search import plot_fpr_vs_residual_threshold\n",
    "from uncertify.evaluation.evaluation_pipeline import run_residual_threshold_evaluation, EvaluationResult, PixelAnomalyDetectionResult, SliceAnomalyDetectionResults, OODDetectionResults\n",
    "from uncertify.evaluation.configs import EvaluationConfig, PixelThresholdSearchConfig\n",
    "try:\n",
    "    tqdm._instances.clear()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "eval_cfg = EvaluationConfig()\n",
    "eval_cfg.use_n_batches = 10\n",
    "results = EvaluationResult(EVAL_DIR_PATH, eval_cfg, PixelAnomalyDetectionResult(), SliceAnomalyDetectionResults(), OODDetectionResults())\n",
    "results.make_dirs()\n",
    "\n",
    "results = run_residual_threshold_evaluation(model, camcan_train_dataloader, eval_cfg, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot MNIST reconstructions\n",
    "Run various MNIST examples (batches consisting of samples of a certain number) through the model and plot input and reconstructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_n_batches = 1\n",
    "batch_size = 8\n",
    "for n in range(0, 10):\n",
    "    _, mnist_val_dataloader = dataloader_factory(DatasetType.MNIST, \n",
    "                                                 batch_size=batch_size, \n",
    "                                                 transform=torchvision.transforms.Compose([\n",
    "                                                                        torchvision.transforms.Resize((128, 128)),\n",
    "                                                                        torchvision.transforms.ToTensor()]),\n",
    "                                                 mnist_label=n)\n",
    "    batch_generator = yield_inference_batches(mnist_val_dataloader, model, residual_threshold=1.8)\n",
    "    plot_stacked_scan_reconstruction_batches(batch_generator, plot_n_batches, \n",
    "                                             cmap='hot', axis='off', figsize=(15, 15), save_dir_path=DATA_DIR_PATH/'reconstructions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot latent space sample reconstructions from different locations in latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uncertify.visualization.latent_space_analysis import plot_latent_samples_from_ring\n",
    "\n",
    "radii = [(0, 1), (2, 3), (4, 5), (7, 9), (10, 12), (15, 17), (20, 30), (50, 60), (200, 210)]\n",
    "\n",
    "for sample in radii:\n",
    "    inner_radius, outer_radius = sample\n",
    "    fig = plot_latent_samples_from_ring(model, n_samples=16, inner_radius=inner_radius, outer_radius=outer_radius)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
